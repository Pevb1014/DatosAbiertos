{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pevb1014/DatosAbiertos/blob/main/Procesamiento_De_Datos_Pablo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gwaUpalqw2v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VislLzh9Cd5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procesamiento PR Señal Horizontal.csv**"
      ],
      "metadata": {
        "id": "zEt7iOhsv1kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar que la cantidad de separadores en todos los registros sea igual\n",
        "\n",
        "bad_rows = []\n",
        "with open(\"/content/drive/MyDrive/Documentos/Pablo/Proyectos/Datos abiertos/Datos crudos/Otros Datos Crudos/PR Señal Horizontal.csv\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    for i, line in enumerate(f, start=1):\n",
        "        num_commas = line.count(\",\")\n",
        "        bad_rows.append((i, num_commas))\n",
        "\n",
        "# calcular moda de comas normales\n",
        "from statistics import mode\n",
        "expected_commas = mode([c for _, c in bad_rows])\n",
        "\n",
        "# buscar filas que se salen del estándar\n",
        "errores = [(i, c) for i, c in bad_rows if c != expected_commas]\n",
        "\n",
        "print(\"Filas problemáticas:\")\n",
        "for i, c in errores[:]:\n",
        "    print(f\"Línea {i}: tiene {c} comas (esperadas {expected_commas})\")\n"
      ],
      "metadata": {
        "id": "kZcncwHW12by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar el dataset y ver informacion basica de su contenido\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Documentos/Pablo/Proyectos/Datos abiertos/Datos crudos/Otros Datos Crudos/PR Señal Horizontal.csv\")\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "D0eZJQ3W4Zj5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Eliminacion de las columnas sin relevancia\n",
        "\n",
        "df = df.drop(columns=[\"objectid\", \"codigo\", \"ubicacion\", \"clasificac\", \"tipo_pintu\", \"territoria\", \"shape_leng\", \"z\", \"st_length(shape)\"])"
      ],
      "metadata": {
        "id": "TBk_5E9mxRwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Manejo de valores nulos en cada columna\n",
        "#Eliminacion de registros que no tienen pr_inicial y pr_final a la vez\n",
        "#Eliminar regisros sin via\n",
        "\n",
        "df = df[~(\n",
        "    (df[\"via\"].isna() | (df[\"via\"].str.strip() == \"\")) |\n",
        "    (df[\"pr_inicial\"].isna() & df[\"pr_final\"].isna())\n",
        ")]\n",
        "\n",
        "#Si pr_inicial es nulo y es mayor a 0 se coloca: pr_inicial = pr_final - 1\n",
        "mask = df[\"pr_inicial\"].isna() & df[\"pr_final\"].notna()\n",
        "\n",
        "df.loc[mask, \"pr_inicial\"] = df.loc[mask, \"pr_final\"].apply(\n",
        "    lambda x: x-1 if x > 0 else 0\n",
        ")\n",
        "#Si pr_final es nulo se coloca: pr_final = pr_inicial 1 1\n",
        "mask2 = df[\"pr_final\"].isna() & df[\"pr_inicial\"].notna()\n",
        "\n",
        "df.loc[mask2, \"pr_final\"] = df.loc[mask2, \"pr_inicial\"].apply(\n",
        "    lambda x: x + 1\n",
        ")\n"
      ],
      "metadata": {
        "id": "CJdLl9RFzl4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtencion del año del contrato\n",
        "df[\"numero_contrato\"] = df[\"contrato\"].str.split(\"_\").str[0]\n",
        "df[\"anio_contrato\"]   = df[\"contrato\"].str.split(\"_\").str[1]\n",
        "df.drop(columns=[\"contrato\"], inplace=True)\n"
      ],
      "metadata": {
        "id": "WlMSZSVI_HS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Descarga dataset procesado a archivo excel\n",
        "df.to_excel(\"PR Señal Horizontal Limpio.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "4TKsmYy5x4dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procesamiento Postes Referencia**"
      ],
      "metadata": {
        "id": "2wfWSyJ1CJyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar que la cantidad de separadores en todos los registros sea igual\n",
        "\n",
        "bad_rows = []\n",
        "with open(\"/content/drive/MyDrive/Documentos/Pablo/Proyectos/Datos abiertos/Datos crudos/Otros Datos Crudos/Postes Referencia.csv\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    for i, line in enumerate(f, start=1):\n",
        "        num_commas = line.count(\",\")\n",
        "        bad_rows.append((i, num_commas))\n",
        "\n",
        "# calcular moda de comas normales\n",
        "from statistics import mode\n",
        "expected_commas = mode([c for _, c in bad_rows])\n",
        "\n",
        "# buscar filas que se salen del estándar\n",
        "errores = [(i, c) for i, c in bad_rows if c != expected_commas]\n",
        "\n",
        "print(\"Filas problemáticas:\")\n",
        "for i, c in errores[:20]:\n",
        "    print(f\"Línea {i}: tiene {c} comas (esperadas {expected_commas})\")\n"
      ],
      "metadata": {
        "id": "evZZJH9TCak2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar el dataset y ver informacion basica de su contenido\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Documentos/Pablo/Proyectos/Datos abiertos/Datos crudos/Otros Datos Crudos/Postes Referencia.csv\")\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "5q-3TKeMDZ5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=[\"objectid\", \"distancia\", \"lado\", \"material\", \"fechainstalacion\", \"fuente\", \"fechafuente\", \"altura\", \"estadociclovida\"])\n",
        "df[\"postereferencia\"] = pd.to_numeric(df[\"postereferencia\"], errors=\"coerce\")\n",
        "df[\"estado\"] = pd.to_numeric(df[\"estado\"], errors=\"coerce\").astype(\"Int64\")\n",
        "df[\"latitud\"] = pd.to_numeric(df[\"latitud\"], errors=\"coerce\")\n",
        "df[\"longitud\"] = pd.to_numeric(df[\"longitud\"], errors=\"coerce\")\n",
        "df[\"observacion\"] = df[\"observacion\"].fillna(\"\").astype(str).str.strip()\n",
        "df = df.dropna(subset=[\"latitud\", \"longitud\"])\n",
        "\n",
        "#Verificar que la longitud y latitud estan en el rango de las de Colombia\n",
        "df = df[\n",
        "    (df[\"latitud\"].between(-4.3, 13.5)) &\n",
        "    (df[\"longitud\"].between(-82, -66))\n",
        "]\n",
        "#Agregar la columna maps asociando link a la ubicacion\n",
        "df[\"maps\"] = (\n",
        "    '=HYPERLINK(\"https://www.google.com/maps?q=' +\n",
        "    df[\"latitud\"].astype(str) + \",\" +\n",
        "    df[\"longitud\"].astype(str) +\n",
        "    '\", \"Ver en Maps\")'\n",
        ")"
      ],
      "metadata": {
        "id": "SSwy5hxIAPbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(\"Postes Referencia Limpio.xlsx\", index=False)\n"
      ],
      "metadata": {
        "id": "-4M4B9wQGB8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procesamiento PR Señal Vertical.csv**"
      ],
      "metadata": {
        "id": "BePwPKkEE89f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar que la cantidad de separadores en todos los registros sea igual\n",
        "\n",
        "bad_rows = []\n",
        "with open(\"/content/drive/MyDrive/Documentos/Pablo/Proyectos/Datos abiertos/Datos crudos/Otros Datos Crudos/PR Señal Vertical.csv\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "    for i, line in enumerate(f, start=1):\n",
        "        num_commas = line.count(\",\")\n",
        "        bad_rows.append((i, num_commas))\n",
        "\n",
        "# calcular moda de comas normales\n",
        "from statistics import mode\n",
        "expected_commas = mode([c for _, c in bad_rows])\n",
        "\n",
        "# buscar filas que se salen del estándar\n",
        "errores = [(i, c) for i, c in bad_rows if c != expected_commas]\n",
        "\n",
        "print(\"Filas problemáticas:\")\n",
        "for i, c in errores[:20]:\n",
        "    print(f\"Línea {i}: tiene {c} comas (esperadas {expected_commas})\")"
      ],
      "metadata": {
        "id": "q60OCpkTFF-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar el dataset y ver informacion basica de su contenido\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Documentos/Pablo/Proyectos/Datos abiertos/Datos crudos/Otros Datos Crudos/PR Señal Vertical.csv\")\n",
        "df.info()\n"
      ],
      "metadata": {
        "id": "UbWR0E8YFQmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Eliminacion de las columnas sin relevancia\n",
        "\n",
        "df = df.drop(columns=[\"objectid\", \"codigo\", \"lado\", \"tipo_senal\", \"codigo_sen\", \"forma_sena\", \"territoria\", \"estado\", \"limpieza\", \"cimiento\", \"soporte\", \"visibilida\", \"mat_placa\", \"sustento\", \"z\"])"
      ],
      "metadata": {
        "id": "eEgudUekGg3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Eliminar regisros sin via o sin pr\n",
        "\n",
        "df = df[~(\n",
        "    (df[\"via\"].isna() | (df[\"via\"].str.strip() == \"\")) |\n",
        "    df[\"pr\"].isna()\n",
        ")]\n",
        "\n",
        "# Transformar fecha_insp de timestamp en milisegundos a fecha de Colombia\n",
        "df[\"fecha_insp\"] = (\n",
        "    pd.to_datetime(df[\"fecha_insp\"], unit=\"ms\", utc=True)\n",
        "      .dt.tz_convert(\"America/Bogota\")\n",
        "      .dt.date\n",
        ")\n",
        "\n",
        "#Obtencion del año del contrato\n",
        "df[\"numero_contrato\"] = df[\"contrato\"].str.split(\"_\").str[0]\n",
        "df[\"anio_contrato\"]   = df[\"contrato\"].str.split(\"_\").str[1]\n",
        "df.drop(columns=[\"contrato\"], inplace=True)"
      ],
      "metadata": {
        "id": "_AkwjvkJIpIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(\"PR Señal Vertical Limpio.xlsx\", index=False)"
      ],
      "metadata": {
        "id": "JMGswe7PLF_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procesamiento SIV Señal Vertical"
      ],
      "metadata": {
        "id": "sxyIgPX0l99I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Verificar lineas con columnas estra o faltantes\n",
        "import csv\n",
        "\n",
        "csv_file = \"/content/drive/MyDrive/Documentos/Pablo/Proyectos/Datos abiertos/Datos crudos/Datos Limpios/SIV Señal Vertical.csv\"\n",
        "\n",
        "with open(csv_file, encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    rows = list(reader)\n",
        "\n",
        "# Número de columnas que debería tener cada registro\n",
        "expected_cols = len(rows[0])\n",
        "\n",
        "broken_rows = []\n",
        "\n",
        "for i, row in enumerate(rows):\n",
        "    if len(row) != expected_cols:\n",
        "        broken_rows.append((i+1, row))\n",
        "\n",
        "print(\"Filas con problemas:\")\n",
        "for idx, r in broken_rows:\n",
        "    print(f\" - Línea {idx}: {r}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sfpB1C9zfUM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Identificar registros divididos en varias lineas\n",
        "import string\n",
        "\n",
        "csv_file = \"/content/drive/MyDrive/Documentos/Pablo/Proyectos/Datos abiertos/Datos crudos/Datos Limpios/SIV Señal Vertical.csv\"\n",
        "\n",
        "with open(csv_file, encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "broken = []\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    raw = line.rstrip(\"\\n\")\n",
        "    stripped = raw.strip()\n",
        "\n",
        "    # Saltar encabezado (línea 1)\n",
        "    if i == 0:\n",
        "        continue\n",
        "\n",
        "    # --- REGLAS DE DETECCIÓN ---\n",
        "\n",
        "    # 1. Línea vacía\n",
        "    if stripped == \"\":\n",
        "        broken.append((i+1, raw))\n",
        "        continue\n",
        "\n",
        "    # 2. Línea que comienza con coma\n",
        "    if stripped.startswith(\",\"):\n",
        "        broken.append((i+1, raw))\n",
        "        continue\n",
        "\n",
        "    # 3. Línea que comienza con espacio\n",
        "    if raw.startswith(\" \"):\n",
        "        broken.append((i+1, raw))\n",
        "        continue\n",
        "\n",
        "    # 4. Línea sin comas\n",
        "    if \",\" not in raw:\n",
        "        broken.append((i+1, raw))\n",
        "        continue\n",
        "\n",
        "    # 5. Línea cuyo primer carácter NO es un número\n",
        "    first_char = stripped[0]\n",
        "    if not first_char.isdigit():\n",
        "        broken.append((i+1, raw))\n",
        "        continue\n",
        "\n",
        "print(\"Líneas sospechosas:\")\n",
        "for idx, content in broken:\n",
        "    print(f\" - Línea {idx}: {content}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "BkSIAFdqfo0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unir los registros partidos\n",
        "import csv\n",
        "\n",
        "csv_file = \"/content/drive/MyDrive/Documentos/Pablo/Proyectos/Datos abiertos/Datos crudos/Datos Limpios/SIV Señal Vertical.csv\"\n",
        "output_file = \"data_reconstruido.csv\"\n",
        "\n",
        "with open(csv_file, encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "reconstructed = []\n",
        "current_row = None\n",
        "\n",
        "def is_continuation(line, is_header=False):\n",
        "    raw = line.rstrip(\"\\n\")\n",
        "    stripped = raw.strip()\n",
        "\n",
        "    # No aplicar reglas a encabezado\n",
        "    if is_header:\n",
        "        return False\n",
        "\n",
        "    # --- REGLAS DE DETECCIÓN DE CONTINUACIÓN ---\n",
        "    if stripped == \"\":\n",
        "        return True\n",
        "\n",
        "    if stripped.startswith(\",\"):\n",
        "        return True\n",
        "\n",
        "    if raw.startswith(\" \"):\n",
        "        return True\n",
        "\n",
        "    if \",\" not in raw:\n",
        "        return True\n",
        "\n",
        "    if not stripped[0].isdigit():  # NO empieza con número\n",
        "        return True\n",
        "\n",
        "    # Si pasa todo, es una línea válida\n",
        "    return False\n",
        "\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    is_header = (i == 0)\n",
        "    raw = line.rstrip(\"\\n\")\n",
        "\n",
        "    if is_continuation(raw, is_header=is_header):\n",
        "        # Esta línea es continuación → concatenarla\n",
        "        if current_row is None:\n",
        "            # Si por alguna razón aparece un fragmento antes de cualquier registro válido\n",
        "            current_row = raw\n",
        "        else:\n",
        "            current_row += \" \" + raw.strip()\n",
        "    else:\n",
        "        # Esta línea es un registro válido → empezar uno nuevo\n",
        "        if current_row is not None:\n",
        "            reconstructed.append(current_row)\n",
        "        current_row = raw  # iniciar un nuevo registro\n",
        "\n",
        "# Agregar el último registro si existe\n",
        "if current_row is not None:\n",
        "    reconstructed.append(current_row)\n",
        "\n",
        "\n",
        "# --- Guardar resultado reconstruido ---\n",
        "with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    for row in reconstructed:\n",
        "        f.write(row + \"\\n\")\n",
        "\n",
        "print(f\"Reconstrucción completa. Total registros: {len(reconstructed)}\")\n",
        "print(f\"Archivo generado: {output_file}\")\n"
      ],
      "metadata": {
        "id": "-JvxZNXSiEbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "input_file = \"/content/data_reconstruido.csv\"\n",
        "output_file = \"SIV Señal Vertical.csv\"\n",
        "\n",
        "clean_rows = []\n",
        "\n",
        "with open(input_file, encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    rows = list(reader)\n",
        "\n",
        "# Número correcto de columnas según encabezado\n",
        "expected_cols = len(rows[0])\n",
        "\n",
        "# Índice de la columna de texto (detección automática opcional)\n",
        "# → Si ya sabes cuál es, ponlo manual.\n",
        "# Aquí asumo que el texto es la ÚLTIMA columna ANTES de donde empiezan a romperse las filas.\n",
        "# Si prefieres indicar un índice específico, me lo dices.\n",
        "text_col_index = expected_cols - 10   # Ajusta si el texto está en otra posición\n",
        "\n",
        "for row in rows:\n",
        "    # Caso perfecto: longitud correcta\n",
        "    if len(row) == expected_cols:\n",
        "        clean_rows.append(row)\n",
        "        continue\n",
        "\n",
        "    # Caso con columnas de más → texto roto por comas internas\n",
        "    if len(row) > expected_cols:\n",
        "        # Tomamos las columnas buenas iniciales\n",
        "        fixed = row[:expected_cols]\n",
        "\n",
        "        # Partes extra que son fragmentos del texto roto\n",
        "        extra = row[expected_cols - 1 :]\n",
        "\n",
        "        # Unir fragmentos del texto, quitando comas internas\n",
        "        merged_text = \" \".join(extra).replace(\",\", \" \")\n",
        "\n",
        "        # Insertar texto limpio en la columna correcta\n",
        "        fixed[text_col_index] = merged_text\n",
        "\n",
        "        clean_rows.append(fixed)\n",
        "        continue\n",
        "\n",
        "    # Si tiene menos columnas (casos raros):\n",
        "    # Aquí puedes mejorar según tu caso.\n",
        "    clean_rows.append(row)\n",
        "\n",
        "\n",
        "# Guardar archivo limpio\n",
        "with open(output_file, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(clean_rows)\n",
        "\n",
        "print(\"Archivo corregido:\", output_file)\n",
        "print(\"Columnas esperadas:\", expected_cols)\n"
      ],
      "metadata": {
        "id": "tbx4f9wVjmE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# =============================\n",
        "#  CONFIG\n",
        "# =============================\n",
        "input_csv = \"/content/SIV Señal Vertical.csv\"\n",
        "temp_reconstructed = \"/content/data_reconstruido.csv\"\n",
        "output_clean = \"/content/SIV Señal Vertical_Limpio.csv\"\n",
        "\n",
        "# Índice donde está tu columna de texto (0 = primera columna)\n",
        "text_col_index = -10   # AJUSTA según tu archivo\n",
        "\n",
        "# =============================\n",
        "#  1. LEER CSV ORIGINAL\n",
        "# =============================\n",
        "with open(input_csv, encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# =============================\n",
        "#  2. DETECTAR Y RECONSTRUIR REGISTROS PARTIDOS\n",
        "# =============================\n",
        "\n",
        "def is_continuation(line, is_header=False):\n",
        "    raw = line.rstrip(\"\\n\")\n",
        "    stripped = raw.strip()\n",
        "\n",
        "    if is_header:\n",
        "        return False\n",
        "\n",
        "    # Reglas de que es una continuación\n",
        "    if stripped == \"\":\n",
        "        return True\n",
        "    if stripped.startswith(\",\"):\n",
        "        return True\n",
        "    if raw.startswith(\" \"):\n",
        "        return True\n",
        "    if \",\" not in raw:\n",
        "        return True\n",
        "    if not stripped[0].isdigit():\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "reconstructed = []\n",
        "current_row = None\n",
        "\n",
        "for i, line in enumerate(lines):\n",
        "    is_header = (i == 0)\n",
        "    raw = line.rstrip(\"\\n\")\n",
        "\n",
        "    if is_continuation(raw, is_header=is_header):\n",
        "        if current_row is None:\n",
        "            current_row = raw\n",
        "        else:\n",
        "            current_row += \" \" + raw.strip()\n",
        "    else:\n",
        "        if current_row is not None:\n",
        "            reconstructed.append(current_row)\n",
        "        current_row = raw\n",
        "\n",
        "if current_row is not None:\n",
        "    reconstructed.append(current_row)\n",
        "\n",
        "# Guardar reconstrucción temporal\n",
        "with open(temp_reconstructed, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    for row in reconstructed:\n",
        "        f.write(row + \"\\n\")\n",
        "\n",
        "print(f\"✔ Registros reconstruidos: {len(reconstructed)}\\nArchivo: {temp_reconstructed}\")\n",
        "\n",
        "\n",
        "# =============================\n",
        "#  3. CORREGIR COMAS INTERNAS EN COLUMNAS\n",
        "# =============================\n",
        "with open(temp_reconstructed, encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    rows = list(reader)\n",
        "\n",
        "expected_cols = len(rows[0])  # columnas esperadas según encabezado\n",
        "clean_rows = []\n",
        "\n",
        "for row in rows:\n",
        "    if len(row) == expected_cols:\n",
        "        clean_rows.append(row)\n",
        "        continue\n",
        "\n",
        "    if len(row) > expected_cols:\n",
        "        fixed = row[:expected_cols]\n",
        "        extra = row[expected_cols - 1:]\n",
        "        merged_text = \" \".join(extra).replace(\",\", \" \")\n",
        "        fixed[text_col_index] = merged_text\n",
        "        clean_rows.append(fixed)\n",
        "        continue\n",
        "\n",
        "    clean_rows.append(row)\n",
        "\n",
        "\n",
        "# =============================\n",
        "#  4. GUARDAR RESULTADO FINAL\n",
        "# =============================\n",
        "with open(output_clean, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(clean_rows)\n",
        "\n",
        "print(\"\\n==============================\")\n",
        "print(\"✔ ARCHIVO FINAL GENERADO\")\n",
        "print(\"==============================\")\n",
        "print(\"Archivo limpio:\", output_clean)\n",
        "print(\"Columnas esperadas:\", expected_cols)\n"
      ],
      "metadata": {
        "id": "aKZVuVC-p2oE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}